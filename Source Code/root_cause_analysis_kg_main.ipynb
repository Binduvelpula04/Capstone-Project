{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XDMoLnxVZjAp"
   },
   "source": [
    "# ðŸ” Root Cause Analysis using Knowledge Graphs & LLM\n",
    "\n",
    "## Complete 4-Phase Workflow:\n",
    "1. **Data Preprocessing** - Load, clean, and merge event and performance data\n",
    "2. **Knowledge Graph Construction** - Build entity-relationship graph\n",
    "3. **Causal Analysis & Inference** - Detect root causes using correlations and graph algorithms\n",
    "4. **LLM Integration** - Query the knowledge graph using natural language\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UqGRBDRQZjAq"
   },
   "source": [
    "## Install Required Dependencies\n",
    "\n",
    "Install all necessary packages for data analysis, graph processing, visualization, and LLM integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eGFJWw4rZjAq",
    "outputId": "05d8cf6b-d443-4089-c11c-58e82948c730"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install pandas numpy networkx matplotlib seaborn pyvis scipy scikit-learn -q\n",
    "!pip install transformers torch sentence-transformers -q\n",
    "!pip install plotly kaleido -q\n",
    "\n",
    "print(\"All dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_d6HJJYjZjAr"
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hS8kgZFrZjAr",
    "outputId": "480ce905-79bd-405e-8a33-77794f2748f7"
   },
   "outputs": [],
   "source": [
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Graph libraries\n",
    "import networkx as nx\n",
    "from pyvis.network import Network\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "# Statistical analysis\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# LLM Integration\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(f\"PyTorch CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H4Iiw-6SZjAr"
   },
   "source": [
    "---\n",
    "# PHASE 1: Data Understanding & Preprocessing\n",
    "\n",
    "Load both CSV files, explore the data structure, and perform comprehensive cleaning and feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1QAkWRFsZjAr"
   },
   "source": [
    "## 1.1 Load Data Files\n",
    "\n",
    "**Note:** Upload `event.csv` and `perf.csv` to your Colab session first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G5k4GqtQZjAs",
    "outputId": "82386e8b-3c6f-4875-8319-eb5116a0c54f"
   },
   "outputs": [],
   "source": [
    "# Load the CSV files\n",
    "# If files are in Google Drive, mount and update paths accordingly\n",
    "try:\n",
    "    events_df = pd.read_csv('/content/event.csv')\n",
    "    perf_df = pd.read_csv('/content/perf.csv')\n",
    "    print(\"Data loaded successfully!\")\n",
    "    print(f\"\\n Events data shape: {events_df.shape}\")\n",
    "    print(f\"Performance data shape: {perf_df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Files not found. Please upload 'event.csv' and 'perf.csv' to Colab.\")\n",
    "    print(\"Run the following to upload:\")\n",
    "    print(\"from google.colab import files\")\n",
    "    print(\"uploaded = files.upload()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7M3n9gULZjAs"
   },
   "source": [
    "## 1.2 Initial Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "lm_uhv-lZjAs",
    "outputId": "787bcfd8-e888-4c83-c145-a835d8ff787a"
   },
   "outputs": [],
   "source": [
    "# Display first few rows of events data\n",
    "print(\"=\" * 80)\n",
    "print(\"EVENT DATA - First 5 rows\")\n",
    "print(\"=\" * 80)\n",
    "display(events_df.head())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EVENT DATA - Info\")\n",
    "print(\"=\" * 80)\n",
    "print(events_df.info())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EVENT DATA - Summary Statistics\")\n",
    "print(\"=\" * 80)\n",
    "display(events_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "jerCJ9FmZjAs",
    "outputId": "35ce288b-520f-4f7a-bb97-6fa7dc06beca"
   },
   "outputs": [],
   "source": [
    "# Display first few rows of performance data\n",
    "print(\"=\" * 80)\n",
    "print(\"PERFORMANCE DATA - First 5 rows\")\n",
    "print(\"=\" * 80)\n",
    "display(perf_df.head())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PERFORMANCE DATA - Info\")\n",
    "print(\"=\" * 80)\n",
    "print(perf_df.info())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PERFORMANCE DATA - Summary Statistics\")\n",
    "print(\"=\" * 80)\n",
    "display(perf_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dPRUBsL-ZjAt"
   },
   "source": [
    "## 1.3 Data Cleaning & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KmOkfHf0ZjAt",
    "outputId": "60022212-34d8-457b-f278-a7cc3e43b3be"
   },
   "outputs": [],
   "source": [
    "# Function to clean and preprocess events data\n",
    "def clean_events_data(df):\n",
    "    \"\"\"\n",
    "    Clean and preprocess events data:\n",
    "    - Handle missing values\n",
    "    - Normalize timestamps\n",
    "    - Extract key features\n",
    "    - Encode categories\n",
    "    \"\"\"\n",
    "    df_clean = df.copy()\n",
    "\n",
    "    # 1. Convert timestamp to datetime\n",
    "    df_clean['TimeGenerated'] = pd.to_datetime(df_clean['TimeGenerated [UTC]'], errors='coerce')\n",
    "\n",
    "    # 2. Handle missing values in key columns\n",
    "    df_clean['EventLevelName'].fillna('Unknown', inplace=True)\n",
    "    df_clean['RenderedDescription'].fillna('No description', inplace=True)\n",
    "    df_clean['Computer'].fillna('Unknown', inplace=True)\n",
    "\n",
    "    # 3. Remove duplicates\n",
    "    df_clean.drop_duplicates(subset=['TimeGenerated', 'EventID', 'Computer'], keep='first', inplace=True)\n",
    "\n",
    "    # 4. Extract severity level (numeric encoding)\n",
    "    severity_map = {'Critical': 5, 'Error': 4, 'Warning': 3, 'Information': 2, 'Verbose': 1, 'Unknown': 0}\n",
    "    df_clean['Severity'] = df_clean['EventLevelName'].map(severity_map)\n",
    "\n",
    "    # 5. Create event category from Source\n",
    "    df_clean['EventCategory'] = df_clean['Source'].str.split('-').str[-1]\n",
    "\n",
    "    # 6. Sort by timestamp\n",
    "    df_clean.sort_values('TimeGenerated', inplace=True)\n",
    "    df_clean.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return df_clean\n",
    "\n",
    "# Apply cleaning\n",
    "events_clean = clean_events_data(events_df)\n",
    "print(\"Events data cleaned!\")\n",
    "print(f\"Original shape: {events_df.shape} â†’ Cleaned shape: {events_clean.shape}\")\n",
    "print(f\"Removed {len(events_df) - len(events_clean)} duplicate rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jYtOlVFeZjAt",
    "outputId": "d5669ae3-b589-4f76-dfe8-160d856bee89"
   },
   "outputs": [],
   "source": [
    "# Function to clean and preprocess performance data\n",
    "def clean_perf_data(df):\n",
    "    \"\"\"\n",
    "    Clean and preprocess performance data:\n",
    "    - Handle missing values\n",
    "    - Normalize timestamps\n",
    "    - Create metric identifiers\n",
    "    \"\"\"\n",
    "    df_clean = df.copy()\n",
    "\n",
    "    # 1. Convert timestamp to datetime\n",
    "    df_clean['TimeGenerated'] = pd.to_datetime(df_clean['TimeGenerated [UTC]'], errors='coerce')\n",
    "\n",
    "    # 2. Handle missing values\n",
    "    df_clean['InstanceName'].fillna('_Total', inplace=True)\n",
    "    df_clean['CounterValue'] = pd.to_numeric(df_clean['CounterValue'], errors='coerce')\n",
    "    df_clean.dropna(subset=['CounterValue'], inplace=True)\n",
    "\n",
    "    # 3. Create composite metric name\n",
    "    df_clean['MetricName'] = df_clean['ObjectName'] + '_' + df_clean['CounterName'].str.replace(' ', '_')\n",
    "\n",
    "    # 4. Remove duplicates\n",
    "    df_clean.drop_duplicates(subset=['TimeGenerated', 'MetricName', 'Computer'], keep='first', inplace=True)\n",
    "\n",
    "    # 5. Sort by timestamp\n",
    "    df_clean.sort_values('TimeGenerated', inplace=True)\n",
    "    df_clean.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return df_clean\n",
    "\n",
    "# Apply cleaning\n",
    "perf_clean = clean_perf_data(perf_df)\n",
    "print(\"Performance data cleaned!\")\n",
    "print(f\"Original shape: {perf_df.shape} â†’ Cleaned shape: {perf_clean.shape}\")\n",
    "print(f\"Removed {len(perf_df) - len(perf_clean)} rows with missing/invalid values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S_C2t5opZjAt"
   },
   "source": [
    "## 1.4 Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y7tZ1lByZjAt",
    "outputId": "45dc8917-f676-41bc-8e96-36ca07fbeec8"
   },
   "outputs": [],
   "source": [
    "# Feature engineering for events data\n",
    "def engineer_event_features(df):\n",
    "    \"\"\"\n",
    "    Create advanced features from events data:\n",
    "    - Event frequency\n",
    "    - Time between events\n",
    "    - Event bursts\n",
    "    \"\"\"\n",
    "    df_feat = df.copy()\n",
    "\n",
    "    # 1. Calculate time difference between consecutive events\n",
    "    df_feat['TimeDelta'] = df_feat['TimeGenerated'].diff().dt.total_seconds()\n",
    "\n",
    "    # 2. Event frequency per minute\n",
    "    df_feat['EventsPerMinute'] = df_feat.groupby(df_feat['TimeGenerated'].dt.floor('1min'))['EventID'].transform('count')\n",
    "\n",
    "    # 3. Rolling event count (last 5 events)\n",
    "    df_feat['RollingEventCount'] = df_feat['EventID'].rolling(window=5, min_periods=1).count()\n",
    "\n",
    "    # 4. Identify event bursts (more than 10 events per minute)\n",
    "    df_feat['EventBurst'] = (df_feat['EventsPerMinute'] > 10).astype(int)\n",
    "\n",
    "    # 5. Mean time between events by EventID\n",
    "    df_feat['MeanTimeBetweenEvents'] = df_feat.groupby('EventID')['TimeDelta'].transform('mean')\n",
    "\n",
    "    return df_feat\n",
    "\n",
    "events_featured = engineer_event_features(events_clean)\n",
    "print(\"Event features engineered!\")\n",
    "print(f\"New columns added: {set(events_featured.columns) - set(events_clean.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "84etp2dLZjAu",
    "outputId": "a2bbbf63-cbc1-4f2f-f33f-a9ff00681443"
   },
   "outputs": [],
   "source": [
    "# Feature engineering for performance data\n",
    "def engineer_perf_features(df):\n",
    "    \"\"\"\n",
    "    Create advanced features from performance data:\n",
    "    - Rolling averages\n",
    "    - Deviations from baseline\n",
    "    - Threshold alerts\n",
    "    \"\"\"\n",
    "    df_feat = df.copy()\n",
    "\n",
    "    # Create pivot table for easier feature engineering\n",
    "    pivot_df = df_feat.pivot_table(\n",
    "        index=['TimeGenerated', 'Computer'],\n",
    "        columns='MetricName',\n",
    "        values='CounterValue',\n",
    "        aggfunc='mean'\n",
    "    ).reset_index()\n",
    "\n",
    "    # Calculate rolling statistics for key metrics\n",
    "    for col in pivot_df.columns[2:]:  # Skip TimeGenerated and Computer\n",
    "        if pivot_df[col].dtype in ['float64', 'int64']:\n",
    "            # Rolling mean (5 periods)\n",
    "            pivot_df[f'{col}_RollingMean'] = pivot_df[col].rolling(window=5, min_periods=1).mean()\n",
    "\n",
    "            # Deviation from mean\n",
    "            pivot_df[f'{col}_Deviation'] = pivot_df[col] - pivot_df[col].mean()\n",
    "\n",
    "            # Z-score for anomaly detection\n",
    "            pivot_df[f'{col}_ZScore'] = (pivot_df[col] - pivot_df[col].mean()) / (pivot_df[col].std() + 1e-10)\n",
    "\n",
    "    return pivot_df\n",
    "\n",
    "perf_featured = engineer_perf_features(perf_clean)\n",
    "print(\"Performance features engineered!\")\n",
    "print(f\"Total features created: {len(perf_featured.columns)}\")\n",
    "print(f\"Sample features: {list(perf_featured.columns[:10])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hcQ4Mi56ZjAu"
   },
   "source": [
    "## 1.5 Merge Events and Performance Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 414
    },
    "id": "AvzMWly-ZjAu",
    "outputId": "12b04e67-1dbf-4e35-e2fc-5eb4a21b4007"
   },
   "outputs": [],
   "source": [
    "# Merge datasets on time windows (nearest timestamp match)\n",
    "def merge_event_perf_data(events_df, perf_df, time_window='1min'):\n",
    "    \"\"\"\n",
    "    Merge events and performance data based on time windows.\n",
    "    Groups performance data by time window and matches with events.\n",
    "    \"\"\"\n",
    "    # Round timestamps to nearest time window\n",
    "    events_df['TimeWindow'] = events_df['TimeGenerated'].dt.floor(time_window)\n",
    "    perf_df['TimeWindow'] = perf_df['TimeGenerated'].dt.floor(time_window)\n",
    "\n",
    "    # Aggregate events by time window\n",
    "    events_agg = events_df.groupby(['TimeWindow', 'Computer']).agg({\n",
    "        'EventID': 'count',\n",
    "        'Severity': 'max',\n",
    "        'EventBurst': 'max',\n",
    "        'EventsPerMinute': 'mean'\n",
    "    }).rename(columns={'EventID': 'EventCount'}).reset_index()\n",
    "\n",
    "    # Merge on time window and computer\n",
    "    merged_df = pd.merge(\n",
    "        events_agg,\n",
    "        perf_df,\n",
    "        on=['TimeWindow', 'Computer'],\n",
    "        how='outer'\n",
    "    )\n",
    "\n",
    "    # Fill missing values\n",
    "    merged_df['EventCount'].fillna(0, inplace=True)\n",
    "    merged_df['Severity'].fillna(0, inplace=True)\n",
    "    merged_df['EventBurst'].fillna(0, inplace=True)\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "unified_data = merge_event_perf_data(events_featured, perf_featured)\n",
    "print(\"Data merged successfully!\")\n",
    "print(f\"Unified dataset shape: {unified_data.shape}\")\n",
    "print(f\"\\nTime range: {unified_data['TimeWindow'].min()} to {unified_data['TimeWindow'].max()}\")\n",
    "display(unified_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Aax-f_xcZjAu"
   },
   "source": [
    "## 1.6 Save Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "08Mja0jbZjAu",
    "outputId": "8a44e26d-1ff8-47e9-ddd5-d3668d4cd0c1"
   },
   "outputs": [],
   "source": [
    "# Save cleaned and processed data\n",
    "events_featured.to_csv('events_cleaned.csv', index=False)\n",
    "perf_featured.to_csv('perf_cleaned.csv', index=False)\n",
    "unified_data.to_csv('unified_data.csv', index=False)\n",
    "\n",
    "print(\"  - Cleaned data saved!\")\n",
    "print(\"  - events_cleaned.csv\")\n",
    "print(\"  - perf_cleaned.csv\")\n",
    "print(\"  - unified_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AgKq2DyJZjAu"
   },
   "source": [
    "---\n",
    "# PHASE 2: Knowledge Graph Construction\n",
    "\n",
    "Build a Knowledge Graph to represent relationships between system entities, events, and metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_W5SGD2EZjAv"
   },
   "source": [
    "## 2.1 Define Knowledge Graph Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cLI2GIkXZjAv",
    "outputId": "72b48d60-5631-4c84-985e-6abee941eb37"
   },
   "outputs": [],
   "source": [
    "# Define entity types and relationships\n",
    "ENTITY_TYPES = {\n",
    "    'System': 'Computer systems',\n",
    "    'Component': 'System components (CPU, Memory, Disk, Network)',\n",
    "    'Event': 'System events and alerts',\n",
    "    'Metric': 'Performance metrics',\n",
    "    'Alert': 'High-severity events'\n",
    "}\n",
    "\n",
    "RELATIONSHIP_TYPES = {\n",
    "    'CAUSES': 'Entity A causes Entity B',\n",
    "    'CORRELATES_WITH': 'Entity A correlates with Entity B',\n",
    "    'AFFECTS': 'Entity A affects Entity B',\n",
    "    'OCCURS_IN': 'Event occurs in System',\n",
    "    'PRECEDES': 'Event A happens before Event B'\n",
    "}\n",
    "\n",
    "print(\"Knowledge Graph Schema Defined\")\n",
    "print(\"\\nEntity Types:\")\n",
    "for entity, desc in ENTITY_TYPES.items():\n",
    "    print(f\"  â€¢ {entity}: {desc}\")\n",
    "\n",
    "print(\"\\nRelationship Types:\")\n",
    "for rel, desc in RELATIONSHIP_TYPES.items():\n",
    "    print(f\"  â€¢ {rel}: {desc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eUUBqFn5ZjAv"
   },
   "source": [
    "## 2.2 Build Knowledge Graph using NetworkX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k4l0czppZjAv",
    "outputId": "5150ef99-1e28-475d-e210-515528bb9f0f"
   },
   "outputs": [],
   "source": [
    "# Initialize Knowledge Graph\n",
    "KG = nx.DiGraph()\n",
    "\n",
    "print(\"Building Knowledge Graph...\")\n",
    "\n",
    "# Add system nodes\n",
    "systems = events_featured['Computer'].unique()\n",
    "for system in systems:\n",
    "    KG.add_node(system, node_type='System', label=system)\n",
    "\n",
    "print(f\"Added {len(systems)} System nodes\")\n",
    "\n",
    "# Add event nodes (sample top 50 most frequent events)\n",
    "top_events = events_featured['EventID'].value_counts().head(50)\n",
    "for event_id, count in top_events.items():\n",
    "    event_name = f\"Event_{event_id}\"\n",
    "    # Get event description\n",
    "    event_desc = events_featured[events_featured['EventID'] == event_id]['RenderedDescription'].iloc[0]\n",
    "    # Truncate description\n",
    "    event_desc = event_desc[:100] + \"...\" if len(event_desc) > 100 else event_desc\n",
    "\n",
    "    KG.add_node(\n",
    "        event_name,\n",
    "        node_type='Event',\n",
    "        label=event_name,\n",
    "        description=event_desc,\n",
    "        frequency=int(count)\n",
    "    )\n",
    "\n",
    "print(f\"Added {len(top_events)} Event nodes\")\n",
    "\n",
    "# Add metric nodes (from performance data)\n",
    "metrics = perf_clean['MetricName'].unique()[:30]  # Top 30 metrics\n",
    "for metric in metrics:\n",
    "    KG.add_node(metric, node_type='Metric', label=metric)\n",
    "\n",
    "print(f\"Added {len(metrics)} Metric nodes\")\n",
    "\n",
    "# Add component nodes\n",
    "components = perf_clean['ObjectName'].unique()\n",
    "for component in components:\n",
    "    KG.add_node(component, node_type='Component', label=component)\n",
    "\n",
    "print(f\"Added {len(components)} Component nodes\")\n",
    "\n",
    "print(f\"\\n Total nodes: {KG.number_of_nodes()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pkHOH53dZjAv"
   },
   "source": [
    "## 2.3 Create Relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fETD5d0mZjAv",
    "outputId": "2e15656f-f870-4937-c215-d9b928f24bf2"
   },
   "outputs": [],
   "source": [
    "# Create OCCURS_IN relationships (Events -> Systems)\n",
    "for _, row in events_featured.iterrows():\n",
    "    event_name = f\"Event_{row['EventID']}\"\n",
    "    system = row['Computer']\n",
    "\n",
    "    if KG.has_node(event_name) and KG.has_node(system):\n",
    "        if not KG.has_edge(event_name, system):\n",
    "            KG.add_edge(event_name, system, relationship='OCCURS_IN', weight=1)\n",
    "        else:\n",
    "            # Increment weight for repeated occurrences\n",
    "            KG[event_name][system]['weight'] += 1\n",
    "\n",
    "print(f\"Created OCCURS_IN relationships\")\n",
    "\n",
    "# Create AFFECTS relationships (Components -> Metrics)\n",
    "for _, row in perf_clean.iterrows():\n",
    "    component = row['ObjectName']\n",
    "    metric = row['MetricName']\n",
    "\n",
    "    if KG.has_node(component) and KG.has_node(metric):\n",
    "        if not KG.has_edge(component, metric):\n",
    "            KG.add_edge(component, metric, relationship='AFFECTS', weight=1)\n",
    "\n",
    "print(f\"Created AFFECTS relationships\")\n",
    "\n",
    "# Create temporal PRECEDES relationships (Event -> Event)\n",
    "events_sorted = events_featured.sort_values('TimeGenerated')\n",
    "for i in range(len(events_sorted) - 1):\n",
    "    event1 = f\"Event_{events_sorted.iloc[i]['EventID']}\"\n",
    "    event2 = f\"Event_{events_sorted.iloc[i+1]['EventID']}\"\n",
    "\n",
    "    time_diff = (events_sorted.iloc[i+1]['TimeGenerated'] - events_sorted.iloc[i]['TimeGenerated']).total_seconds()\n",
    "\n",
    "    # Only connect events that occur within 60 seconds of each other\n",
    "    if time_diff <= 60 and KG.has_node(event1) and KG.has_node(event2):\n",
    "        if not KG.has_edge(event1, event2):\n",
    "            KG.add_edge(event1, event2, relationship='PRECEDES', weight=1, time_diff=time_diff)\n",
    "        else:\n",
    "            KG[event1][event2]['weight'] += 1\n",
    "\n",
    "print(f\"Created PRECEDES relationships\")\n",
    "\n",
    "print(f\"\\n Total edges: {KG.number_of_edges()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KcZdQyW-ZjAv"
   },
   "source": [
    "## 2.4 Visualize Knowledge Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 725
    },
    "id": "GeB2_DOsZjAv",
    "outputId": "8a6177dd-6641-4374-806f-82fe87ceab52"
   },
   "outputs": [],
   "source": [
    "# Visualize using matplotlib (static view)\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Use spring layout for better visualization\n",
    "pos = nx.spring_layout(KG, k=0.5, iterations=50, seed=42)\n",
    "\n",
    "# Color nodes by type\n",
    "node_colors = []\n",
    "for node in KG.nodes():\n",
    "    node_type = KG.nodes[node].get('node_type', 'Unknown')\n",
    "    if node_type == 'System':\n",
    "        node_colors.append('#FF6B6B')  # Red\n",
    "    elif node_type == 'Event':\n",
    "        node_colors.append('#4ECDC4')  # Teal\n",
    "    elif node_type == 'Metric':\n",
    "        node_colors.append('#95E1D3')  # Light green\n",
    "    elif node_type == 'Component':\n",
    "        node_colors.append('#F38181')  # Pink\n",
    "    else:\n",
    "        node_colors.append('#CCCCCC')  # Gray\n",
    "\n",
    "# Draw nodes\n",
    "nx.draw_networkx_nodes(KG, pos, node_color=node_colors, node_size=300, alpha=0.8)\n",
    "\n",
    "# Draw edges\n",
    "nx.draw_networkx_edges(KG, pos, alpha=0.3, arrows=True, arrowsize=10)\n",
    "\n",
    "# Draw labels (only for important nodes)\n",
    "important_nodes = {node: KG.nodes[node].get('label', node)\n",
    "                   for node in KG.nodes()\n",
    "                   if KG.nodes[node].get('node_type') in ['System', 'Component']}\n",
    "nx.draw_networkx_labels(KG, pos, important_nodes, font_size=8)\n",
    "\n",
    "plt.title('Knowledge Graph - System Events & Performance', fontsize=16, fontweight='bold')\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.savefig('knowledge_graph_static.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Static graph visualization saved as 'knowledge_graph_static.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-mce6bk5ZjAw",
    "outputId": "5bd1ab3b-877b-4783-c48a-4659fc07ea56"
   },
   "outputs": [],
   "source": [
    "# Create interactive visualization using PyVis\n",
    "def create_interactive_graph(graph, output_file='knowledge_graph.html'):\n",
    "    \"\"\"\n",
    "    Create an interactive HTML visualization of the knowledge graph.\n",
    "    \"\"\"\n",
    "    net = Network(height='800px', width='100%', bgcolor='#222222', font_color='white', directed=True)\n",
    "    net.barnes_hut(gravity=-8000, central_gravity=0.3, spring_length=200)\n",
    "\n",
    "    # Add nodes with colors based on type\n",
    "    for node in graph.nodes():\n",
    "        node_data = graph.nodes[node]\n",
    "        node_type = node_data.get('node_type', 'Unknown')\n",
    "        label = node_data.get('label', node)\n",
    "\n",
    "        # Set color based on node type\n",
    "        color_map = {\n",
    "            'System': '#FF6B6B',\n",
    "            'Event': '#4ECDC4',\n",
    "            'Metric': '#95E1D3',\n",
    "            'Component': '#F38181',\n",
    "            'Unknown': '#CCCCCC'\n",
    "        }\n",
    "\n",
    "        net.add_node(\n",
    "            node,\n",
    "            label=label,\n",
    "            title=f\"Type: {node_type}\\nNode: {label}\",\n",
    "            color=color_map.get(node_type, '#CCCCCC')\n",
    "        )\n",
    "\n",
    "    # Add edges\n",
    "    for edge in graph.edges(data=True):\n",
    "        source, target, data = edge\n",
    "        relationship = data.get('relationship', 'related')\n",
    "        weight = data.get('weight', 1)\n",
    "\n",
    "        net.add_edge(\n",
    "            source,\n",
    "            target,\n",
    "            title=f\"{relationship} (weight: {weight})\",\n",
    "            value=weight\n",
    "        )\n",
    "\n",
    "    # Save and display\n",
    "    net.save_graph(output_file)\n",
    "    print(f\"Interactive graph saved as '{output_file}'\")\n",
    "    return net\n",
    "\n",
    "interactive_graph = create_interactive_graph(KG)\n",
    "print(\"\\n You can open 'knowledge_graph.html' in your browser to explore the interactive graph.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pgVW6k59ZjAw"
   },
   "source": [
    "## 2.5 Save Knowledge Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_BbpMHsnZjAw",
    "outputId": "ed2c40ef-5049-4800-a02e-fd8f1134f7df"
   },
   "outputs": [],
   "source": [
    "# Save graph to GEXF format (compatible with Gephi and other tools)\n",
    "nx.write_gexf(KG, 'knowledge_graph.gexf')\n",
    "print(\"Knowledge Graph saved as 'knowledge_graph.gexf'\")\n",
    "\n",
    "# Also save as GraphML\n",
    "nx.write_graphml(KG, 'knowledge_graph.graphml')\n",
    "print(\"Knowledge Graph saved as 'knowledge_graph.graphml'\")\n",
    "\n",
    "# Print graph statistics\n",
    "print(\"\\n Knowledge Graph Statistics:\")\n",
    "print(f\"  â€¢ Total Nodes: {KG.number_of_nodes()}\")\n",
    "print(f\"  â€¢ Total Edges: {KG.number_of_edges()}\")\n",
    "print(f\"  â€¢ Graph Density: {nx.density(KG):.4f}\")\n",
    "print(f\"  â€¢ Is Directed: {KG.is_directed()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B9kqZ3yLZjAw"
   },
   "source": [
    "---\n",
    "# PHASE 3: Root Cause & Causal Inference\n",
    "\n",
    "Analyze the Knowledge Graph to identify root causes using correlation analysis and graph algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3okTJXHOZjAw"
   },
   "source": [
    "## 3.1 Compute Correlations Between Events and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1jQ3vBT7ZjAw",
    "outputId": "153353f0-af6e-4ef8-d10b-ac6698eb82a0"
   },
   "outputs": [],
   "source": [
    "# Create time-series data for correlation analysis\n",
    "def prepare_correlation_data(events_df, perf_df, time_window='5min'):\n",
    "    \"\"\"\n",
    "    Prepare data for correlation analysis by aggregating over time windows.\n",
    "    \"\"\"\n",
    "    # Aggregate events\n",
    "    events_df['TimeWindow'] = events_df['TimeGenerated'].dt.floor(time_window)\n",
    "    event_counts = events_df.groupby(['TimeWindow', 'EventID']).size().reset_index(name='EventCount')\n",
    "    event_pivot = event_counts.pivot(index='TimeWindow', columns='EventID', values='EventCount').fillna(0)\n",
    "    event_pivot.columns = [f'Event_{col}' for col in event_pivot.columns]\n",
    "\n",
    "    # Aggregate performance metrics\n",
    "    perf_df['TimeWindow'] = perf_df['TimeGenerated'].dt.floor(time_window)\n",
    "    perf_agg = perf_df.pivot_table(\n",
    "        index='TimeWindow',\n",
    "        columns='MetricName',\n",
    "        values='CounterValue',\n",
    "        aggfunc='mean'\n",
    "    ).fillna(method='ffill').fillna(0)\n",
    "\n",
    "    # Merge\n",
    "    combined = event_pivot.join(perf_agg, how='outer').fillna(0)\n",
    "\n",
    "    return combined\n",
    "\n",
    "correlation_data = prepare_correlation_data(events_featured, perf_clean)\n",
    "print(f\"Correlation data prepared with shape: {correlation_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4__n0oPTZjAw",
    "outputId": "941cb210-36cb-493e-903d-5ceb2f8d37a1"
   },
   "outputs": [],
   "source": [
    "# Compute correlation matrix\n",
    "print(\"Computing correlations...\")\n",
    "correlation_matrix = correlation_data.corr()\n",
    "\n",
    "# Extract event-metric correlations\n",
    "event_cols = [col for col in correlation_matrix.columns if col.startswith('Event_')]\n",
    "metric_cols = [col for col in correlation_matrix.columns if not col.startswith('Event_')]\n",
    "\n",
    "# Get cross-correlations (events vs metrics)\n",
    "cross_corr = correlation_matrix.loc[event_cols, metric_cols]\n",
    "\n",
    "print(f\"Correlation matrix computed: {cross_corr.shape}\")\n",
    "\n",
    "# Visualize top correlations\n",
    "plt.figure(figsize=(14, 10))\n",
    "top_corr = cross_corr.abs().stack().nlargest(70)\n",
    "top_corr_df = pd.DataFrame({\n",
    "    'Event-Metric Pair': [f\"{idx[0]} â†” {idx[1]}\" for idx in top_corr.index],\n",
    "    'Correlation': [cross_corr.loc[idx] for idx in top_corr.index]\n",
    "})\n",
    "\n",
    "sns.barplot(data=top_corr_df, y='Event-Metric Pair', x='Correlation', palette='coolwarm')\n",
    "plt.title('Top 30 Event-Metric Correlations', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Correlation Coefficient')\n",
    "plt.tight_layout()\n",
    "plt.savefig('top_correlations.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Correlation analysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kc018o-4Eiuz"
   },
   "outputs": [],
   "source": [
    "# ---- PREP 1: build correlation_data ----\n",
    "def prepare_correlation_data(events_df, perf_df, time_window='5min'):\n",
    "    events_df = events_df.copy()\n",
    "    perf_df = perf_df.copy()\n",
    "\n",
    "    # Aggregate events\n",
    "    events_df['TimeWindow'] = events_df['TimeGenerated'].dt.floor(time_window)\n",
    "    event_counts = (\n",
    "        events_df.groupby(['TimeWindow', 'EventID'])\n",
    "                 .size()\n",
    "                 .reset_index(name='EventCount')\n",
    "    )\n",
    "    event_pivot = event_counts.pivot(index='TimeWindow',\n",
    "                                     columns='EventID',\n",
    "                                     values='EventCount').fillna(0)\n",
    "    event_pivot.columns = [f'Event_{col}' for col in event_pivot.columns]\n",
    "\n",
    "    # Aggregate performance metrics\n",
    "    perf_df['TimeWindow'] = perf_df['TimeGenerated'].dt.floor(time_window)\n",
    "    perf_agg = perf_df.pivot_table(\n",
    "        index='TimeWindow',\n",
    "        columns='MetricName',\n",
    "        values='CounterValue',\n",
    "        aggfunc='mean'\n",
    "    ).fillna(method='ffill').fillna(0)\n",
    "\n",
    "    # Merge\n",
    "    combined = event_pivot.join(perf_agg, how='outer').fillna(0)\n",
    "    combined.sort_index(inplace=True)\n",
    "    return combined\n",
    "\n",
    "correlation_data = prepare_correlation_data(events_featured, perf_clean)\n",
    "\n",
    "# ---- PREP 2: correlations (events vs metrics) ----\n",
    "corr = correlation_data.corr()\n",
    "event_cols  = [c for c in corr.columns if c.startswith('Event_')]\n",
    "metric_cols = [c for c in corr.columns if not c.startswith('Event_')]\n",
    "\n",
    "cross_corr = corr.loc[event_cols, metric_cols]\n",
    "\n",
    "# Long-form pairs\n",
    "pairs = cross_corr.stack().reset_index()\n",
    "pairs.columns = ['Event', 'Metric', 'Correlation']\n",
    "pairs['abs_corr'] = pairs['Correlation'].abs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9uE9je13Eiu0",
    "outputId": "42f27de4-a59d-4fd5-faab-85c524a3fa11"
   },
   "outputs": [],
   "source": [
    "# Pick the top 3â€“4 most variable metrics\n",
    "metric_variance = correlation_data[metric_cols].var().sort_values(ascending=False)\n",
    "top_metrics = metric_variance.head(4).index.tolist()\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "for m in top_metrics:\n",
    "    series = correlation_data[m]\n",
    "    series_norm = (series - series.min()) / (series.max() - series.min() + 1e-9)\n",
    "    plt.plot(series_norm.index, series_norm, label=m)\n",
    "\n",
    "plt.title(\"Figure 1 â€“ Key Metrics Over Time (normalized)\", fontsize=14, fontweight=\"bold\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Normalized Value (0â€“1)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pjOIuZ-SEiu1",
    "outputId": "dc2d6f9d-4316-4f82-f357-1603f78b4de2"
   },
   "outputs": [],
   "source": [
    "# Total events per time window\n",
    "event_counts_ts = correlation_data[event_cols].sum(axis=1)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(event_counts_ts.index, event_counts_ts.values)\n",
    "plt.title(\"Figure 2 â€“ Total Event Activity Over Time\", fontsize=14, fontweight=\"bold\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Number of Events (per 5 min)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3l40PsPNEiu1",
    "outputId": "9729c4ea-cdd2-41ee-fb50-03695e4c0d5b"
   },
   "outputs": [],
   "source": [
    "# ---- PREP 3: build event bundles based on co-occurrence ----\n",
    "event_only = correlation_data[event_cols]\n",
    "event_corr = event_only.corr()\n",
    "\n",
    "threshold = 0.99   # treat events with corr â‰¥ 0.99 as a bundle\n",
    "visited = set()\n",
    "bundles = []\n",
    "\n",
    "for ev in event_corr.index:\n",
    "    if ev in visited:\n",
    "        continue\n",
    "    group = event_corr.index[event_corr.loc[ev] >= threshold].tolist()\n",
    "    visited.update(group)\n",
    "    bundles.append(group)\n",
    "\n",
    "# Map each event to a bundle name\n",
    "bundle_map = {}\n",
    "for i, group in enumerate(bundles, start=1):\n",
    "    name = f\"EventBundle_{i}\"\n",
    "    for ev in group:\n",
    "        bundle_map[ev] = name\n",
    "\n",
    "print(\"Event bundles:\")\n",
    "for i, g in enumerate(bundles, start=1):\n",
    "    print(f\"  EventBundle_{i}: {g}\")\n",
    "\n",
    "# Build bundle time series (sum counts in each bundle)\n",
    "bundle_df = pd.DataFrame(index=correlation_data.index)\n",
    "for ev, name in bundle_map.items():\n",
    "    if name not in bundle_df:\n",
    "        bundle_df[name] = 0\n",
    "    bundle_df[name] += correlation_data[ev]\n",
    "\n",
    "metric_only = correlation_data[metric_cols]\n",
    "correlation_data_bundles = pd.concat([bundle_df, metric_only], axis=1)\n",
    "\n",
    "# correlations with bundles\n",
    "corr_b = correlation_data_bundles.corr()\n",
    "bundle_cols = [c for c in corr_b.columns if c.startswith(\"EventBundle_\")]\n",
    "metric_cols_b = [c for c in corr_b.columns if c not in bundle_cols]\n",
    "\n",
    "cross_corr_b = corr_b.loc[bundle_cols, metric_cols_b]\n",
    "pairs_b = cross_corr_b.stack().reset_index()\n",
    "pairs_b.columns = ['Bundle', 'Metric', 'Correlation']\n",
    "pairs_b['abs_corr'] = pairs_b['Correlation'].abs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rDpjLjF0Eiu2",
    "outputId": "ded50a27-a56a-4d7d-99e7-71bd41d2d392"
   },
   "outputs": [],
   "source": [
    "# Strong correlations only\n",
    "strong_b = pairs_b[pairs_b['abs_corr'] >= 0.6]\n",
    "\n",
    "top_bundles = (\n",
    "    strong_b.groupby('Bundle')['abs_corr']\n",
    "            .max()\n",
    "            .sort_values(ascending=False)\n",
    "            .head(6)\n",
    "            .index\n",
    ")\n",
    "\n",
    "top_metrics = (\n",
    "    strong_b.groupby('Metric')['abs_corr']\n",
    "            .max()\n",
    "            .sort_values(ascending=False)\n",
    "            .head(6)\n",
    "            .index\n",
    ")\n",
    "\n",
    "heat_b = strong_b[strong_b['Bundle'].isin(top_bundles) &\n",
    "                  strong_b['Metric'].isin(top_metrics)]\n",
    "\n",
    "heat_pivot_b = heat_b.pivot(index='Bundle', columns='Metric', values='Correlation')\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(heat_pivot_b, annot=True, fmt=\".2f\", center=0, cmap='coolwarm')\n",
    "plt.title(\"Figure 3 â€“ Strong Correlations: Event Bundles vs Metrics (|r| â‰¥ 0.6)\",\n",
    "          fontsize=14, fontweight=\"bold\")\n",
    "plt.xlabel(\"Metric\")\n",
    "plt.ylabel(\"Event Bundle\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qiNRqk82Eiu2",
    "outputId": "89a6ebcb-58b6-4db0-c8b3-005645923d9a"
   },
   "outputs": [],
   "source": [
    "def plot_bundle_metric_timeseries(data_bundles, bundle_col, metric_col):\n",
    "    ts = data_bundles[[bundle_col, metric_col]].copy().sort_index()\n",
    "\n",
    "    # Normalize to 0â€“1 so they share an axis\n",
    "    ts_norm = (ts - ts.min()) / (ts.max() - ts.min() + 1e-9)\n",
    "\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.plot(ts_norm.index, ts_norm[bundle_col], label=bundle_col, linewidth=2)\n",
    "    plt.plot(ts_norm.index, ts_norm[metric_col], label=metric_col,\n",
    "             linestyle=\"--\", linewidth=2)\n",
    "    plt.title(f\"Figure 4 â€“ Time Series: {bundle_col} vs {metric_col}\",\n",
    "              fontsize=14, fontweight=\"bold\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Normalized Value (0â€“1)\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example (use actual names that appear in Figure 3):\n",
    "plot_bundle_metric_timeseries(\n",
    "    correlation_data_bundles,\n",
    "    bundle_col=\"EventBundle_1\",\n",
    "    metric_col=\"LogicalDisk_%_Idle_Time\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9GpDg-5FZjAw"
   },
   "source": [
    "## 3.2 Add Causal Edges to Knowledge Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JJOkSNCIZjAx",
    "outputId": "af4c934d-471e-45c5-eb10-69ea506869a1"
   },
   "outputs": [],
   "source": [
    "# Add causal relationships based on high correlations\n",
    "CORRELATION_THRESHOLD = 0.5  # Threshold for significant correlation\n",
    "\n",
    "causal_edges_added = 0\n",
    "\n",
    "for event in event_cols:\n",
    "    for metric in metric_cols:\n",
    "        corr_value = cross_corr.loc[event, metric]\n",
    "\n",
    "        # Add causal edge if correlation is strong\n",
    "        if abs(corr_value) > CORRELATION_THRESHOLD:\n",
    "            if KG.has_node(event) and KG.has_node(metric):\n",
    "                KG.add_edge(\n",
    "                    event,\n",
    "                    metric,\n",
    "                    relationship='CORRELATES_WITH' if corr_value > 0 else 'INVERSELY_CORRELATED',\n",
    "                    weight=abs(corr_value),\n",
    "                    correlation=corr_value\n",
    "                )\n",
    "                causal_edges_added += 1\n",
    "\n",
    "print(f\"Added {causal_edges_added} causal edges based on correlations\")\n",
    "print(f\"Updated graph: {KG.number_of_nodes()} nodes, {KG.number_of_edges()} edges\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7R3KU9C5ZjAx"
   },
   "source": [
    "## 3.3 Run Graph Algorithms for Root Cause Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cuXe5rbzZjAx",
    "outputId": "af29feb5-ff83-46bd-f596-12adef9271c4"
   },
   "outputs": [],
   "source": [
    "# Calculate PageRank to identify most influential nodes\n",
    "print(\"Running PageRank algorithm...\")\n",
    "pagerank = nx.pagerank(KG, weight='weight')\n",
    "\n",
    "# Sort by PageRank score\n",
    "pagerank_sorted = sorted(pagerank.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"\\n Top 20 Most Influential Nodes (PageRank):\")\n",
    "print(\"=\" * 80)\n",
    "for i, (node, score) in enumerate(pagerank_sorted[:20], 1):\n",
    "    node_type = KG.nodes[node].get('node_type', 'Unknown')\n",
    "    label = KG.nodes[node].get('label', node)\n",
    "    print(f\"{i:2d}. {label:40s} | Type: {node_type:10s} | Score: {score:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tjchPPHfZjAx",
    "outputId": "3ad5e370-a46e-4b1a-dedf-97102a346df6"
   },
   "outputs": [],
   "source": [
    "# Calculate Degree Centrality\n",
    "print(\"\\n Calculating Degree Centrality...\")\n",
    "in_degree = dict(KG.in_degree())\n",
    "out_degree = dict(KG.out_degree())\n",
    "\n",
    "# Sort by in-degree (nodes that are affected by many others)\n",
    "in_degree_sorted = sorted(in_degree.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"\\n Top 15 Nodes by In-Degree (Most Affected):\")\n",
    "print(\"=\" * 80)\n",
    "for i, (node, degree) in enumerate(in_degree_sorted[:15], 1):\n",
    "    node_type = KG.nodes[node].get('node_type', 'Unknown')\n",
    "    label = KG.nodes[node].get('label', node)\n",
    "    print(f\"{i:2d}. {label:40s} | Type: {node_type:10s} | In-Degree: {degree}\")\n",
    "\n",
    "# Sort by out-degree (nodes that affect many others - potential root causes)\n",
    "out_degree_sorted = sorted(out_degree.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"\\n Top 15 Nodes by Out-Degree (Potential Root Causes):\")\n",
    "print(\"=\" * 80)\n",
    "for i, (node, degree) in enumerate(out_degree_sorted[:15], 1):\n",
    "    node_type = KG.nodes[node].get('node_type', 'Unknown')\n",
    "    label = KG.nodes[node].get('label', node)\n",
    "    print(f\"{i:2d}. {label:40s} | Type: {node_type:10s} | Out-Degree: {degree}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hfghxuObZjAy",
    "outputId": "83865b7b-f7a4-4987-e645-b84d17c33c9b"
   },
   "outputs": [],
   "source": [
    "# Calculate Betweenness Centrality (nodes that connect different parts of the graph)\n",
    "print(\"\\n Calculating Betweenness Centrality...\")\n",
    "betweenness = nx.betweenness_centrality(KG, weight='weight')\n",
    "betweenness_sorted = sorted(betweenness.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"\\n Top 15 Nodes by Betweenness Centrality (Critical Bridges):\")\n",
    "print(\"=\" * 80)\n",
    "for i, (node, score) in enumerate(betweenness_sorted[:15], 1):\n",
    "    node_type = KG.nodes[node].get('node_type', 'Unknown')\n",
    "    label = KG.nodes[node].get('label', node)\n",
    "    print(f\"{i:2d}. {label:40s} | Type: {node_type:10s} | Score: {score:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3gTCB_QeZjAy"
   },
   "source": [
    "## 3.4 Identify Top Root Causes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TztMvClCZjAy",
    "outputId": "603ebe74-48e4-41b5-efaf-f63d261cd959"
   },
   "outputs": [],
   "source": [
    "# Combine multiple metrics to identify root causes\n",
    "def identify_root_causes(graph, pagerank, out_degree, betweenness, top_n=10):\n",
    "    \"\"\"\n",
    "    Combine multiple centrality metrics to identify likely root causes.\n",
    "    Focuses on Event nodes with high out-degree and PageRank.\n",
    "    \"\"\"\n",
    "    # Normalize scores\n",
    "    max_pr = max(pagerank.values())\n",
    "    max_od = max(out_degree.values())\n",
    "    max_bc = max(betweenness.values()) if max(betweenness.values()) > 0 else 1\n",
    "\n",
    "    root_cause_scores = {}\n",
    "\n",
    "    for node in graph.nodes():\n",
    "        node_type = graph.nodes[node].get('node_type', 'Unknown')\n",
    "\n",
    "        # Focus on Event nodes as potential root causes\n",
    "        if node_type == 'Event':\n",
    "            # Composite score: weighted combination of metrics\n",
    "            pr_score = pagerank.get(node, 0) / max_pr\n",
    "            od_score = out_degree.get(node, 0) / max_od\n",
    "            bc_score = betweenness.get(node, 0) / max_bc\n",
    "\n",
    "            # Weighted combination (out-degree is most important for root causes)\n",
    "            composite_score = (0.4 * od_score) + (0.3 * pr_score) + (0.3 * bc_score)\n",
    "\n",
    "            root_cause_scores[node] = {\n",
    "                'composite_score': composite_score,\n",
    "                'pagerank': pr_score,\n",
    "                'out_degree': out_degree.get(node, 0),\n",
    "                'betweenness': bc_score,\n",
    "                'label': graph.nodes[node].get('label', node),\n",
    "                'description': graph.nodes[node].get('description', 'N/A')\n",
    "            }\n",
    "\n",
    "    # Sort by composite score\n",
    "    sorted_causes = sorted(root_cause_scores.items(), key=lambda x: x[1]['composite_score'], reverse=True)\n",
    "\n",
    "    return sorted_causes[:top_n]\n",
    "\n",
    "top_root_causes = identify_root_causes(KG, pagerank, out_degree, betweenness, top_n=10)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"TOP 10 LIKELY ROOT CAUSES\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "for i, (node, metrics) in enumerate(top_root_causes, 1):\n",
    "    print(f\"\\n{i}. {metrics['label']}\")\n",
    "    print(f\"   Description: {metrics['description']}\")\n",
    "    print(f\"   Composite Score: {metrics['composite_score']:.4f}\")\n",
    "    print(f\"   Out-Degree: {metrics['out_degree']} (affects {metrics['out_degree']} other entities)\")\n",
    "    print(f\"   PageRank: {metrics['pagerank']:.4f}\")\n",
    "    print(f\"   Betweenness: {metrics['betweenness']:.4f}\")\n",
    "    print(f\"   -\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "60lvBTJ_ZjAy"
   },
   "source": [
    "## 3.5 Trace Impact Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_2Ky-Os-ZjAy",
    "outputId": "d2975592-9068-493b-dcb5-ea49239015ad"
   },
   "outputs": [],
   "source": [
    "# Trace paths from root causes to impacted metrics\n",
    "def trace_impact_paths(graph, root_cause, target_type='Metric', max_depth=3):\n",
    "    \"\"\"\n",
    "    Find all paths from a root cause to nodes of a specific type.\n",
    "    \"\"\"\n",
    "    target_nodes = [n for n in graph.nodes() if graph.nodes[n].get('node_type') == target_type]\n",
    "\n",
    "    paths = []\n",
    "    for target in target_nodes:\n",
    "        try:\n",
    "            # Find shortest path\n",
    "            path = nx.shortest_path(graph, source=root_cause, target=target)\n",
    "            if len(path) <= max_depth + 1:\n",
    "                paths.append(path)\n",
    "        except nx.NetworkXNoPath:\n",
    "            continue\n",
    "\n",
    "    return paths\n",
    "\n",
    "# Trace paths for top root cause\n",
    "if top_root_causes:\n",
    "    top_cause_node = top_root_causes[0][0]\n",
    "    top_cause_label = top_root_causes[0][1]['label']\n",
    "\n",
    "    impact_paths = trace_impact_paths(KG, top_cause_node, target_type='Metric', max_depth=3)\n",
    "\n",
    "    print(f\"\\n Impact Paths from '{top_cause_label}':\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    for i, path in enumerate(impact_paths[:10], 1):  # Show first 10 paths\n",
    "        path_str = \" â†’ \".join([KG.nodes[n].get('label', n) for n in path])\n",
    "        print(f\"{i}. {path_str}\")\n",
    "\n",
    "    if len(impact_paths) > 10:\n",
    "        print(f\"\\n... and {len(impact_paths) - 10} more paths\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HPD0N-zJZjAy"
   },
   "source": [
    "---\n",
    "# PHASE 4: LLM Integration for Querying\n",
    "\n",
    "Integrate a free, lightweight LLM for natural language querying of the knowledge graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fVN3hg2B4NXH",
    "outputId": "c7e85aa3-08bf-48c7-f655-8b83cb549f8c"
   },
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "\n",
    "# 4.1 Shared system prompt and prompt builder\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a root cause analysis assistant.\n",
    "You work over a knowledge graph of events, components, and performance metrics.\n",
    "Use only the facts given in the Knowledge section.\n",
    "If the graph does not contain enough information, say that clearly.\n",
    "Be concise and clear.\n",
    "\"\"\".strip()\n",
    "\n",
    "def build_user_prompt(query: str, relevant_triples) -> str:\n",
    "    \"\"\"\n",
    "    Build the user-side content (knowledge + question) that will be sent to any LLM.\n",
    "    \"\"\"\n",
    "    bullets = \"\\n\".join(f\"- {t['text']}\" for t in relevant_triples)\n",
    "    return (\n",
    "        \"You are given facts from an IT knowledge graph.\\n\"\n",
    "        \"Use them to answer the question as clearly as possible.\\n\\n\"\n",
    "        f\"Knowledge:\\n{bullets}\\n\\n\"\n",
    "        f\"Question: {query}\\n\"\n",
    "        \"Answer:\"\n",
    "    )\n",
    "\n",
    "## 4.2 Convert Graph to Natural Language Triples\n",
    "\n",
    "def extract_knowledge_triples(graph, max_triples: int = 500) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Convert graph relationships into simple natural language triples.\n",
    "\n",
    "    Each triple is a dictionary with:\n",
    "      - subject\n",
    "      - relationship\n",
    "      - object\n",
    "      - text   (natural language form)\n",
    "      - weight (edge weight if present, else 1)\n",
    "    \"\"\"\n",
    "    # Initialize an empty list to store all triples\n",
    "    triples: List[Dict[str, Any]] = []\n",
    "\n",
    "    # Iterate over all edges of the graph, including edge data\n",
    "    for source, target, data in graph.edges(data=True):\n",
    "        # Get a human readable label for the source node\n",
    "        # If there is no label, fall back to the node id\n",
    "        source_label = graph.nodes[source].get(\"label\", source)\n",
    "\n",
    "        # Same idea for the target node\n",
    "        target_label = graph.nodes[target].get(\"label\", target)\n",
    "\n",
    "        # Determine the relationship type for this edge\n",
    "        # If missing, default to a generic phrase\n",
    "        relationship = data.get(\"relationship\", \"related to\")\n",
    "\n",
    "        # Build a natural language sentence for this triple\n",
    "        # Example: \"CPU spike causes High latency\"\n",
    "        triple_text = f\"{source_label} {relationship.lower().replace('_', ' ')} {target_label}\"\n",
    "\n",
    "        # Append the structured triple record\n",
    "        triples.append(\n",
    "            {\n",
    "                \"subject\": source_label,\n",
    "                \"relationship\": relationship,\n",
    "                \"object\": target_label,\n",
    "                \"text\": triple_text,\n",
    "                \"weight\": data.get(\"weight\", 1),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # If we hit the maximum triple count, stop\n",
    "        if len(triples) >= max_triples:\n",
    "            break\n",
    "\n",
    "    # Return the full list of triples\n",
    "    return triples\n",
    "\n",
    "# Extract triples from your knowledge graph KG\n",
    "knowledge_triples = extract_knowledge_triples(KG, max_triples=500)\n",
    "\n",
    "# Quick sanity check printout\n",
    "print(f\"Extracted {len(knowledge_triples)} knowledge triples\\n\")\n",
    "print(\"Sample triples:\")\n",
    "for triple in knowledge_triples[:10]:\n",
    "    print(f\"  â€¢ {triple['text']}\")\n",
    "\n",
    "# 4. Simple keyword based triple retrieval\n",
    "def query_knowledge_graph(\n",
    "    query: str,\n",
    "    graph,\n",
    "    triples: List[Dict[str, Any]],\n",
    "    top_k: int = 5,\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Retrieve relevant triples for a natural language query.\n",
    "\n",
    "    Retrieval steps:\n",
    "      - Lowercase the query.\n",
    "      - Split the query into individual words.\n",
    "      - Keep any triple whose text contains at least one of these words.\n",
    "      - Sort the matched triples by weight in descending order.\n",
    "      - Return only the top_k triples.\n",
    "    \"\"\"\n",
    "    # Lowercase the query once for reuse\n",
    "    query_lower = query.lower()\n",
    "\n",
    "    # Prepare a list to collect matching triples\n",
    "    relevant_triples: List[Dict[str, Any]] = []\n",
    "\n",
    "    # Loop through all triples\n",
    "    for triple in triples:\n",
    "        # Lowercase the triple text for case insensitive match\n",
    "        triple_text = triple[\"text\"].lower()\n",
    "\n",
    "        # If any query word appears inside the triple text, mark as relevant\n",
    "        if any(word in triple_text for word in query_lower.split()):\n",
    "            relevant_triples.append(triple)\n",
    "\n",
    "    # Sort relevant triples by edge weight, highest first\n",
    "    relevant_triples.sort(key=lambda x: x[\"weight\"], reverse=True)\n",
    "\n",
    "    # Return only the top_k most relevant triples\n",
    "    return relevant_triples[:top_k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nAENcaVDZjAy"
   },
   "source": [
    "## 4.1 Load Free LLM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 330,
     "referenced_widgets": [
      "9a19ad2015444d6a97409b08c78c24e5",
      "929dabe3cef84eea83ce3c1b6e64947c",
      "14583ee09700464c8338fb35265a33ec",
      "ab7888cdd7174b1894db8c11495f65d2",
      "ed65f09f2050465dae7a7df3d2cd744a",
      "8ba2456748524c67ae2585fe4a2bedeb",
      "735c8e2759a0474ca17b667efdc61576",
      "2d3041238432431a8b94639ffcb38df7",
      "fd4818aac09944c39c8a2b8307359a05",
      "67514e9c50a14975b90f9e6b9b516402",
      "3f06fa370e94481b80d3513ccd547876",
      "cb243179619843dbaf91bcf65e954629",
      "f48d4d033e9446dd9906635cffcd660f",
      "f9795868fb9046b0b9380f8f55b79521",
      "cabdaad62aee43e3b82d7587e5a182ae",
      "4c328489db934d5caafec49cbcdd708b",
      "888ffe2b2f294356881a0bc55adb15da",
      "c0c6ff58e1dd4381bd99c438a83ec093",
      "ad2f5f93480c40d7a2cda68b6bf0fbe7",
      "472086fbdb3a4480b64f9112cdbea2df",
      "85f4ad279fb14bd1a639c93183a84a5a",
      "5febc6ee0f12464d83ff6ccd718efe87",
      "2a27701336b04d54814883739e849c39",
      "4120d314cf154a13b5794d51311a2ea4",
      "56b2f59f1f05465f9e03bd283c025fe5",
      "7318f331d9b94a459091ccc112f6a180",
      "c00b2fca95304b628a0f6a7430419c83",
      "b3ae69ef610849b197abf9758a47f79c",
      "ce99feedf0664132832bb055523e82ad",
      "71d2118b589b4ec882d8c2e2dafd1463",
      "7870bc78f5354b2d862a0f2c9755b9a7",
      "0bde07ca2b6e4cad8a9be7c111d7f848",
      "ac37974f8c8f45b3bedea3a86326a838",
      "874c23aefb9644e4bd8d0585771f91d0",
      "a3fee73f306b4e7c97b6202492122276",
      "a60bcac3ce2b43edb10f186b3e53476c",
      "6112662f9f2d4f0da75886d6103bf984",
      "70bfbc7d06b245c6a0db8f34bde2673e",
      "6d64c934e483434587fb5b2c9be1aa7e",
      "d0314b95293d4f3490b6c3c7d2b30ede",
      "8ad831e3e376440883915049718a2ab4",
      "dbe80b640abd47b39b5e224852f1de41",
      "02469e6a86de4b9e8778a675a6b78150",
      "2338d474adcc4974aca17166fcbeedd2",
      "b9f2122cf961407eb80b521aec39cff3",
      "85c4de4cbdb7451c8c81f8296c6513b0",
      "be304285da074e12851102526fb115b7",
      "47eb4faec0d14c4cbbaf89737ae42f53",
      "ddb2f13c9cd5492aaaa5d84140a2ea18",
      "ede61d88f828443e9d2e114a4f0b1002",
      "b872423c64254dd795aa7b61320ffeb4",
      "cc11d4f21bbf4bbbb2e2cd2c50ec573c",
      "920c638d2d1c409e90d17c9120255693",
      "7ac2cb91429f40ea8fcc9e514e9aca0d",
      "99c5976d5279442384c083ceec13f9a2",
      "2166ab3f52f94516aafae108213beff9",
      "67b17d2c57aa4cada1b3cce84f47b087",
      "8e035f83c8464c738ca74b483e708e05",
      "23eae7bb181442d1a5868f0d3084e373",
      "a336bdb9b22846cc844ae5c84e6a0a85",
      "e8f3a7bc2e8447118c0aba211b2bacbb",
      "b38e2480dc78400cbe849ad4dccff563",
      "b42733736fad4e8a84d202860e1fb0f4",
      "b32d931f825a424c88269f4745baa2b5",
      "963c802c66e4423ab1c1342efc699140",
      "6ef95b71ea8d49f7af01d9b25fb725f8",
      "ba826c1f1bc44b12b2047e7c1e77f5d6",
      "d1f76f761bd8470abe802ac85f9f3a67",
      "88a3fbdb204c491f97f4ab95649e9680",
      "c28a89b5d1c340789bd40c5d653955d9",
      "3ca0509e153448758272462e542e0912",
      "61b4d793cd174879acca0593e91a55ae",
      "a457f1fac075482c9738d1c5e9a1ce92",
      "af8c944bcdaf4f0ca8611e7ee2d9fca7",
      "887cf4c9ded2485db0be04c05362a007",
      "dd3e509bd9284a588e7903cbe5624883",
      "b9506e12add6496fa377e4b1322f6749"
     ]
    },
    "id": "m3Pbh7HHZjAz",
    "outputId": "ae583063-18ce-434b-a0df-d2dae579090e"
   },
   "outputs": [],
   "source": [
    "# Load a lightweight text generation model from Hugging Face\n",
    "# Using Google's FLAN-T5 - a free, instruction-following model\n",
    "print(\"Loading FLAN-T5 model (this may take a few minutes)...\")\n",
    "\n",
    "model_name = \"google/flan-t5-base\"  # Lightweight model that runs on Colab\n",
    "\n",
    "# Load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# Create pipeline for easier use\n",
    "llm_pipeline = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=512,\n",
    "    do_sample=False\n",
    ")\n",
    "\n",
    "print(\"LLM loaded successfully!\")\n",
    "print(f\"Model: {model_name}\")\n",
    "print(f\"Parameters: ~{sum(p.numel() for p in model.parameters()) / 1e6:.1f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pE87TZMSZjAz"
   },
   "source": [
    "## 4.2 Build Query Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T3bkpB4RZjAz",
    "outputId": "d202d612-4c39-45d4-8090-24780b6c05b1"
   },
   "outputs": [],
   "source": [
    "# Function to generate natural language answer using LLM\n",
    "def generate_answer_google(query, relevant_triples, llm, system_prompt: str = SYSTEM_PROMPT):\n",
    "    \"\"\"\n",
    "    Generate a natural language answer using the FLAN-T5 pipeline.\n",
    "    \"\"\"\n",
    "    # Build the model input using shared helpers\n",
    "    user_prompt = build_user_prompt(query, relevant_triples)\n",
    "    full_prompt = f\"{system_prompt}\\n\\n{user_prompt}\"\n",
    "\n",
    "    result = llm(full_prompt, max_length=200, do_sample=False)\n",
    "    answer = result[0][\"generated_text\"]\n",
    "    return answer\n",
    "\n",
    "\n",
    "# Main query function\n",
    "def query_graph_llm_google(query_text, graph=KG, triples=knowledge_triples, llm=llm_pipeline):\n",
    "    \"\"\"\n",
    "    Complete query pipeline: retrieve relevant info and generate answer.\n",
    "    \"\"\"\n",
    "    print(f\"\\n Query: {query_text}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Retrieve relevant triples\n",
    "    relevant = query_knowledge_graph(query_text, graph, triples, top_k=10)\n",
    "\n",
    "    if not relevant:\n",
    "        print(\" No relevant information found in the knowledge graph.\")\n",
    "        return None\n",
    "\n",
    "    print(f\"\\n Found {len(relevant)} relevant facts:\")\n",
    "    for i, triple in enumerate(relevant[:5], 1):\n",
    "        print(f\"  {i}. {triple['text']}\")\n",
    "\n",
    "    # Generate answer using LLM\n",
    "    print(\"\\n Generating answer...\")\n",
    "    answer = generate_answer_google(query_text, relevant, llm)\n",
    "\n",
    "    print(\"\\n Answer:\")\n",
    "    print(answer)\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    return answer\n",
    "\n",
    "print(\"Query functions ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nCKD4OgfZjAz"
   },
   "source": [
    "## 4.3 Generate Root Cause Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nQzrRV7UZjAz",
    "outputId": "52c2b02e-8465-400e-c760-a2a004ec1096"
   },
   "outputs": [],
   "source": [
    "# Function to generate a comprehensive root cause report\n",
    "def generate_root_cause_report(top_causes, graph, llm=llm_pipeline):\n",
    "    \"\"\"\n",
    "    Generate a human-readable root cause analysis report.\n",
    "    \"\"\"\n",
    "    report = []\n",
    "    report.append(\"=\" * 100)\n",
    "    report.append(\"ROOT CAUSE ANALYSIS REPORT\")\n",
    "    report.append(\"=\" * 100)\n",
    "    report.append(\"\\n\")\n",
    "\n",
    "    report.append(\"## Executive Summary\\n\")\n",
    "    report.append(f\"Based on analysis of {graph.number_of_nodes()} entities and {graph.number_of_edges()} relationships, \")\n",
    "    report.append(f\"we have identified {len(top_causes)} primary root causes affecting system performance.\\n\\n\")\n",
    "\n",
    "    report.append(\"## Top Root Causes\\n\")\n",
    "\n",
    "    for i, (node, metrics) in enumerate(top_causes[:5], 1):\n",
    "        report.append(f\"\\n### {i}. {metrics['label']}\\n\")\n",
    "        report.append(f\"**Description:** {metrics['description']}\\n\\n\")\n",
    "        report.append(f\"**Impact Score:** {metrics['composite_score']:.4f}\\n\")\n",
    "        report.append(f\"**Affects:** {metrics['out_degree']} downstream entities\\n\\n\")\n",
    "\n",
    "        # Get connected nodes\n",
    "        connected = list(graph.neighbors(node))[:5]\n",
    "        if connected:\n",
    "            report.append(\"**Directly Impacts:**\\n\")\n",
    "            for conn in connected:\n",
    "                conn_label = graph.nodes[conn].get('label', conn)\n",
    "                conn_type = graph.nodes[conn].get('node_type', 'Unknown')\n",
    "                report.append(f\"  - {conn_label} ({conn_type})\\n\")\n",
    "        report.append(\"\\n\")\n",
    "\n",
    "    report.append(\"\\n## Recommendations\\n\\n\")\n",
    "    report.append(\"1. **Monitor high-priority events:** Focus on events with high out-degree centrality.\\n\")\n",
    "    report.append(\"2. **Implement early warning systems:** Set up alerts for root cause events.\\n\")\n",
    "    report.append(\"3. **Review system architecture:** Consider isolating components with high impact.\\n\")\n",
    "    report.append(\"4. **Conduct deeper analysis:** Investigate temporal patterns in root cause events.\\n\")\n",
    "\n",
    "    report.append(\"\\n\" + \"=\" * 100)\n",
    "\n",
    "    full_report = \"\".join(report)\n",
    "    return full_report\n",
    "\n",
    "# Generate and display the report\n",
    "root_cause_report = generate_root_cause_report(top_root_causes, KG)\n",
    "print(root_cause_report)\n",
    "\n",
    "# Save report to file\n",
    "with open('root_cause_report.txt', 'w') as f:\n",
    "    f.write(root_cause_report)\n",
    "\n",
    "print(\"\\n Report saved as 'root_cause_report.txt'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yCA_gdnCZjA0"
   },
   "source": [
    "## 4.4 Example Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "id": "wnK80QaDZjA0",
    "outputId": "fdf5561e-b2bb-4e8f-b14a-927f370b64b4"
   },
   "outputs": [],
   "source": [
    "# Example 1: Query about specific event\n",
    "query_graph_llm_google(\"How many events precedes with another event?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gEtbbxn5ZjA0"
   },
   "source": [
    "---\n",
    "# ðŸ“Š FINAL SUMMARY & VISUALIZATIONS\n",
    "\n",
    "Comprehensive summary of the entire analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3P7O0_uIZjA0",
    "outputId": "578ad35c-19d2-4f46-fa1c-57d069d61b3c"
   },
   "outputs": [],
   "source": [
    "# Create comprehensive summary dashboard\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"ANALYSIS COMPLETE - SUMMARY\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "print(\"\\n Phase 1: Data Preprocessing\")\n",
    "print(f\"  âœ“ Loaded {len(events_df)} events and {len(perf_df)} performance records\")\n",
    "print(f\"  âœ“ Cleaned data: {len(events_clean)} events, {len(perf_clean)} metrics\")\n",
    "print(f\"  âœ“ Created {len(unified_data)} unified time-series records\")\n",
    "\n",
    "print(\"\\n Phase 2: Knowledge Graph\")\n",
    "print(f\"  âœ“ Built graph with {KG.number_of_nodes()} nodes and {KG.number_of_edges()} edges\")\n",
    "print(f\"  âœ“ Entity types: System, Component, Event, Metric\")\n",
    "print(f\"  âœ“ Relationship types: OCCURS_IN, AFFECTS, CORRELATES_WITH, PRECEDES\")\n",
    "\n",
    "print(\"\\n Phase 3: Root Cause Analysis\")\n",
    "print(f\"  âœ“ Computed {cross_corr.size} event-metric correlations\")\n",
    "print(f\"  âœ“ Added {causal_edges_added} causal relationships\")\n",
    "print(f\"  âœ“ Identified {len(top_root_causes)} primary root causes\")\n",
    "\n",
    "print(\"\\n Phase 4: LLM Integration\")\n",
    "print(f\"  âœ“ Loaded {model_name} model\")\n",
    "print(f\"  âœ“ Extracted {len(knowledge_triples)} knowledge triples\")\n",
    "print(f\"  âœ“ Natural language query system ready\")\n",
    "\n",
    "print(\"\\n Generated Files:\")\n",
    "print(\"  â€¢ events_cleaned.csv\")\n",
    "print(\"  â€¢ perf_cleaned.csv\")\n",
    "print(\"  â€¢ unified_data.csv\")\n",
    "print(\"  â€¢ knowledge_graph.gexf\")\n",
    "print(\"  â€¢ knowledge_graph.graphml\")\n",
    "print(\"  â€¢ knowledge_graph.html (interactive)\")\n",
    "print(\"  â€¢ knowledge_graph_static.png\")\n",
    "print(\"  â€¢ top_correlations.png\")\n",
    "print(\"  â€¢ root_cause_report.txt\")\n",
    "\n",
    "print(\"\\n Analysis complete! All deliverables generated.\")\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6FUWkgz2ryo1"
   },
   "source": [
    "# GPT 5 MINI IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dl8QR220r521"
   },
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cE_j7WCKx2Ti"
   },
   "outputs": [],
   "source": [
    "!pip install -q openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y9ctoeDMzgrz"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CHvtFmcGr5NY",
    "outputId": "f79b2542-dcb5-45bd-935d-30e558a5cf31"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Imports and OpenAI client setup\n",
    "\n",
    "from openai import OpenAI                # New style OpenAI client\n",
    "from typing import List, Dict, Any       # For type hints in our functions\n",
    "\n",
    "load_dotenv()  # loads .env file\n",
    "\n",
    "# Create a single shared client instance\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))  # Reads OPENAI_API_KEY from environment\n",
    "\n",
    "print(\"Using OpenAI model: gpt-5-mini\")\n",
    "print(\"OpenAI client ready. Make sure OPENAI_API_KEY is set in the environment.\")\n",
    "\n",
    "# Use OpenAI gpt-5-mini to generate answers\n",
    "def generate_answer_openai(\n",
    "    query: str,\n",
    "    relevant_triples,\n",
    "    model_name: str = \"gpt-5-mini\",\n",
    "    system_prompt: str = SYSTEM_PROMPT,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generate a natural language answer using OpenAI gpt-5-mini.\n",
    "    \"\"\"\n",
    "    # Build user content using the shared function\n",
    "    user_content = build_user_prompt(query, relevant_triples)\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_content},\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    answer_text = completion.choices[0].message.content\n",
    "    return answer_text\n",
    "\n",
    "# Full query pipeline that prints everything nicely\n",
    "def query_graph_llm_openai(\n",
    "    query_text: str,\n",
    "    graph=KG,\n",
    "    triples: List[Dict[str, Any]] = knowledge_triples,\n",
    "    model_name: str = \"gpt-5-mini\",\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    End to end query function.\n",
    "\n",
    "    It will:\n",
    "      - print hello at starting\n",
    "      - Print the query.\n",
    "      - Retrieve relevant triples.\n",
    "      - Print the retrieved facts.\n",
    "      - Call OpenAI gpt-5-mini to generate an answer.\n",
    "      - Print and return the final answer.\n",
    "    \"\"\"\n",
    "    # Show the incoming query\n",
    "    print(f\"\\n Query: {query_text}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Retrieve relevant triples using the basic keyword matcher\n",
    "    relevant = query_knowledge_graph(query_text, graph, triples, top_k=10)\n",
    "\n",
    "    # If no triples match, report and exit early\n",
    "    if not relevant:\n",
    "        print(\"No relevant information found in the knowledge graph.\")\n",
    "        return \"\"\n",
    "\n",
    "    # Print how many relevant facts we got and show the first few\n",
    "    print(f\"\\n Found {len(relevant)} relevant facts:\")\n",
    "    for i, triple in enumerate(relevant[:5], 1):\n",
    "        print(f\"  {i}. {triple['text']}\")\n",
    "\n",
    "    # Ask the OpenAI model for an answer\n",
    "    print(\"\\n Generating answer with OpenAI gpt-5-mini...\")\n",
    "    answer = generate_answer_openai(query_text, relevant, model_name=model_name)\n",
    "\n",
    "    # Print the answer in a clear block\n",
    "    print(\"\\n Answer:\")\n",
    "    print(answer)\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Return the answer string so you can reuse it if needed\n",
    "    return answer\n",
    "\n",
    "print(\"Query functions wired to OpenAI gpt-5-mini.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uuHIRCg8sN8x",
    "outputId": "c53eb385-a03b-48f9-ccfc-f2e4fff4b421"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Root cause report generator\n",
    "#    - Reuses your top_root_causes and KG.\n",
    "def generate_root_cause_report(\n",
    "    top_causes: List[Any],\n",
    "    graph,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generate a human readable root cause analysis report.\n",
    "\n",
    "    Expects:\n",
    "      - top_causes: list of (node_id, metrics_dict)\n",
    "        where metrics_dict has keys:\n",
    "          - label\n",
    "          - description\n",
    "          - composite_score\n",
    "          - out_degree\n",
    "      - graph: NetworkX graph with node attributes.\n",
    "    \"\"\"\n",
    "    # Start with an empty list for report lines\n",
    "    report_lines: List[str] = []\n",
    "\n",
    "    # Header section\n",
    "    report_lines.append(\"=\" * 100 + \"\\n\")\n",
    "    report_lines.append(\"ROOT CAUSE ANALYSIS REPORT\\n\")\n",
    "    report_lines.append(\"=\" * 100 + \"\\n\\n\")\n",
    "\n",
    "    # Executive summary\n",
    "    report_lines.append(\"## Executive Summary\\n\\n\")\n",
    "    report_lines.append(\n",
    "        f\"Based on analysis of {graph.number_of_nodes()} entities and \"\n",
    "        f\"{graph.number_of_edges()} relationships, \"\n",
    "        f\"we have identified {len(top_causes)} primary root causes affecting system performance.\\n\\n\"\n",
    "    )\n",
    "\n",
    "    # Top causes section\n",
    "    report_lines.append(\"## Top Root Causes\\n\")\n",
    "\n",
    "    # Loop over the first five most important causes\n",
    "    for i, (node, metrics) in enumerate(top_causes[:5], 1):\n",
    "        report_lines.append(f\"\\n### {i}. {metrics['label']}\\n\\n\")\n",
    "        report_lines.append(f\"**Description:** {metrics['description']}\\n\\n\")\n",
    "        report_lines.append(f\"**Impact Score:** {metrics['composite_score']:.4f}\\n\\n\")\n",
    "        report_lines.append(f\"**Affects:** {metrics['out_degree']} downstream entities\\n\\n\")\n",
    "\n",
    "        # Show a few directly connected nodes to highlight impact area\n",
    "        connected_nodes = list(graph.neighbors(node))[:5]\n",
    "        if connected_nodes:\n",
    "            report_lines.append(\"**Directly Impacts:**\\n\")\n",
    "            for conn in connected_nodes:\n",
    "                conn_label = graph.nodes[conn].get(\"label\", conn)\n",
    "                conn_type = graph.nodes[conn].get(\"node_type\", \"Unknown\")\n",
    "                report_lines.append(f\"  - {conn_label} ({conn_type})\\n\")\n",
    "        report_lines.append(\"\\n\")\n",
    "\n",
    "    # Recommendation section\n",
    "    report_lines.append(\"\\n## Recommendations\\n\\n\")\n",
    "    report_lines.append(\"1. **Monitor high priority events:** Focus on events with high out degree centrality.\\n\")\n",
    "    report_lines.append(\"2. **Implement early warning systems:** Set up alerts for critical root cause events.\\n\")\n",
    "    report_lines.append(\"3. **Review system architecture:** Consider isolating components with very high impact.\\n\")\n",
    "    report_lines.append(\"4. **Conduct deeper analysis:** Investigate temporal patterns of recurring root cause events.\\n\")\n",
    "\n",
    "    # Footer line\n",
    "    report_lines.append(\"\\n\" + \"=\" * 100 + \"\\n\")\n",
    "\n",
    "    # Combine everything into a single string\n",
    "    full_report = \"\".join(report_lines)\n",
    "\n",
    "    return full_report\n",
    "\n",
    "# Build and save the root cause report\n",
    "root_cause_report = generate_root_cause_report(top_root_causes, KG)\n",
    "\n",
    "# Print the report to the notebook\n",
    "print(root_cause_report)\n",
    "\n",
    "# Save it as a text file inside the Colab environment\n",
    "with open(\"root_cause_report.txt\", \"w\") as f:\n",
    "    f.write(root_cause_report)\n",
    "\n",
    "print(\"\\n Report saved as 'root_cause_report.txt'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "id": "9T7XRYhLz0Qp",
    "outputId": "bb3edf95-942a-4a74-d8a2-9e08b65544d6"
   },
   "outputs": [],
   "source": [
    "# Example 1: Query about specific event\n",
    "query_graph_llm_openai(\"How many events precedes with another event?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XG_s93_aLbK2"
   },
   "source": [
    "# Llama Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 759,
     "referenced_widgets": [
      "9c71d14e1bd743bcbaf1fe0e60eeb932",
      "684fa9d7d3a84310a8361c8a1325e672",
      "54cc8f374c2d4e049b55493cbf8e5e15",
      "f77af6b4eb114af6b7d7b88539c24964",
      "ecd6e08293fd4c2f919b373ff1ae4a96",
      "816aa8e6b12f4b7896a55d951268a179",
      "7f494df9e00f4845b457e1d9910c2d94",
      "a92c924b23d34b209b783890571f7e4c",
      "35b7097e09e34665b1cd9b6497342cc1",
      "15bf44f0dbb24d629834da9a8fda401c",
      "9bc212140a7e4971bd7bcaa26b70e22b",
      "4f37bb99b6a04465b8629e1abe86395c",
      "72ffa04a1d1f4e0f869a2e8738ec5344",
      "bc1d5107d1004f168670060c019e71cc",
      "eda1845bb9bc4ac38fffff68838032c2",
      "9caeb3032d244fff818042c07c3a5921",
      "bf583139ffe9445a8433be9ca8a9f669",
      "2f997c97770a484d843bae476e2a8cb0",
      "323c483329894f53b4c3af701b302016",
      "0910ef546b7441d69fce0010ec4121e4",
      "7469d15b25354becb4d758301059ca7a",
      "159466ee8e1e46b79ad690407cc08b8d",
      "388be2036f9b40539eb29be8565f0b99",
      "75c2c2a011ab4319a02d25c9a564d02f",
      "7983d4e4c2644ff394a6f22d456af543",
      "e7f2f9597174417389b8f7649e6dcf34",
      "864ec8cb167b427a9ac685fd6c24716b",
      "cd98067dfd2d42d99e7ed5fc73f4d4bb",
      "2ead9b80dc274564ba428e80789c63a5",
      "9dea360753fb4008a4ea02bed5232d72",
      "f4bdc9ee697a4ffdbf139908a1916202",
      "8857ea0167fc47ce881e6e1be7a7310c",
      "03568d8c52a84f66b16fb0eae83e70c0",
      "fd4955c357424630a806d2011f7b65c5",
      "eb980d5d0cab4d5db8ebc3a33565f09b",
      "107360a62bb746bf8217c30ee16d8bae",
      "64047809e45a4e7cb8ea4167def2ef5f",
      "fcc328bbb64e4a2fa6af51173626e4fc",
      "18e00b9a869a41cc97c5f302531dfa49",
      "4385f0d0110c4ea5960b9f622fb639d1",
      "cf97236ac2534384ac94881d0dbc1a5c",
      "9d13692a6814474cb196c393869cf749",
      "8aa4fdb4bf2c47b8affe6ec782924655",
      "512c7ca484bd43fc92ec937d7af5df99",
      "ad5af33574bc401594080e9dc57b9ea2",
      "322f87a9820b4e0e99a75135ba20abd4",
      "6b28c29631bd45178d4250c262fb815f",
      "5c2d4076c47b4a68bf014be1124b1653",
      "334aa49ece4d4fedb20d1267941ed30b",
      "7b4c744d50a749cd8160323ed2a22b73",
      "c8921aafbc8349ed87b9b2b5318370fe",
      "efb22033cf7c40d1b2df52d7387280d7",
      "a7ed27fc4e8746ee8f405cbe9bdc98ac",
      "3d4a6026a66347c295dcbee20d21aca9",
      "aca8152d7fa3491d83e384f60354b77b",
      "c1a714808e3343ca9f4c3ba7448d19fe",
      "9723ddbf16194001a1842dfa3ec468ca",
      "541b1f070e1c48df9a911c26cf173a5f",
      "cc57e5d50e754235beb6454dd798895b",
      "d403519bf998417dbe2a02b65a1f9d59",
      "09613b1cf3aa4f8c8bb75c56dba9d811",
      "1318a8d3b1d940449b3501953e3cab59",
      "d07a583dedfd49389497a8250a6e8b69",
      "060b1c6adfbf4918b0aac492ee26a4ce",
      "42aad20b0154489da017fa8ad64192d6",
      "e42e98bb17c94d599f651b4f3d36bc78",
      "b6fd793479fe4655b8aef11852ad5b48",
      "5d701072fb7e4dfa9f78f35202ab3ec5",
      "d7f25bed959041e79abab29851f617e0",
      "033eeb80199c4e8781626467ecb17e1d",
      "17e2684952fd40a18ceb616bc9bffe20",
      "e465cbae10254cee9645e2dd131968e5",
      "ed5d8205ff2344739ed6650f29515ac4",
      "5a99ea0c778e4e6096ab234d37b76133",
      "183824fe33fd443eac2289967ec5146a",
      "7c1928f3a830494d88290132d640dace",
      "e3c46d73e4c34c479fba871291020204"
     ]
    },
    "id": "L5fjSR8OLfvm",
    "outputId": "a8c09981-7275-4b24-959a-1874cd0848ec"
   },
   "outputs": [],
   "source": [
    "# @title TinyLlama Integration (Failsafe Version)\n",
    "# Install libraries (if not already present)\n",
    "!pip install -q accelerate transformers\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "# --- 1. MODEL LOADING ---\n",
    "# TinyLlama-1.1B-Chat: Robust, fast, and no special permissions needed.\n",
    "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "print(f\"Loading {model_id}...\")\n",
    "\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16  # Standard half-precision for GPU\n",
    "    )\n",
    "\n",
    "    # Initialize pipeline\n",
    "    llama_pipeline = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.1,\n",
    "        do_sample=True,\n",
    "        top_p=0.95\n",
    "    )\n",
    "    print(\"TinyLlama model loaded successfully!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "\n",
    "# --- 2. QUERY FUNCTION ---\n",
    "def generate_answer_tinyllama(\n",
    "    query: str,\n",
    "    relevant_triples,\n",
    "    llm,\n",
    "    system_prompt: str = SYSTEM_PROMPT,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Use TinyLlama to answer based on graph triples.\n",
    "    \"\"\"\n",
    "    user_prompt = build_user_prompt(query, relevant_triples)\n",
    "    full_prompt = f\"{system_prompt}\\n\\n{user_prompt}\"\n",
    "\n",
    "    prompt = (\n",
    "        \"<|system|>\\n\" + system_prompt + \"\\n\"\n",
    "        \"<|user|>\\n\" + user_prompt + \"\\n\"\n",
    "        \"<|assistant|>\\n\"\n",
    "    )\n",
    "\n",
    "    outputs = llm(prompt)\n",
    "    generated_text = outputs[0][\"generated_text\"]\n",
    "\n",
    "    if \"<|assistant|>\" in generated_text:\n",
    "        response = generated_text.split(\"<|assistant|>\\n\")[-1].strip()\n",
    "    else:\n",
    "        response = generated_text.strip()\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "def query_graph_with_llm_llama(query_text, graph=KG, triples=knowledge_triples):\n",
    "    print(f\"\\n Query: {query_text}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # 1. Search Knowledge Graph\n",
    "    try:\n",
    "        relevant = query_knowledge_graph(query_text, graph, triples, top_k=15)\n",
    "    except NameError:\n",
    "        print(\"Error: 'query_knowledge_graph' or 'KG' not defined. Run previous cells first.\")\n",
    "        return\n",
    "\n",
    "    if not relevant:\n",
    "        print(\"No relevant information found.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(relevant)} relevant facts.\")\n",
    "\n",
    "    # 2. Generate Answer\n",
    "    try:\n",
    "        answer = generate_answer_tinyllama(query_text, relevant, llama_pipeline)\n",
    "        print(f\"\\nAnswer:\\n{answer}\")\n",
    "    except NameError:\n",
    "        print(\"Model pipeline not initialized.\")\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "# 3. TEST\n",
    "query_graph_with_llm_llama(\"What are the most critical root cause events I should fix first?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tNfdBFrZJAVk",
    "outputId": "a0497c41-55b9-4043-9d20-c917ec1e3f20"
   },
   "outputs": [],
   "source": [
    "# Example 1: Query about specific event flat T5\n",
    "query_graph_llm_google(\"How many events precedes with another event?\")\n",
    "# Example 1: Query about specific event with chat-gpt-openai\n",
    "query_graph_llm_openai(\"How many events precedes with another event?\")\n",
    "# Example 1: Query about specific event with chat-llama\n",
    "query_graph_with_llm_llama(\"How many events precedes with another event?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f8ygHFRKKY-e",
    "outputId": "321e108a-597b-4c8a-9c87-576edb4db554"
   },
   "outputs": [],
   "source": [
    "# Example 2: Query about specific event flat T5\n",
    "query_graph_llm_google(\"What metrics are affected by events?\")\n",
    "# Example 2: Query about specific event with chat-gpt-openai\n",
    "query_graph_llm_openai(\"What metrics are affected by events?\")\n",
    "# Example 2: Query about specific event with chat-llama\n",
    "query_graph_with_llm_llama(\"What metrics are affected by events?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K3u5T909LREB",
    "outputId": "2fa515d6-0bcd-472e-8049-f9d922fcf617"
   },
   "outputs": [],
   "source": [
    "# Example 3: Query about specific event flat T5\n",
    "query_graph_llm_google(\"What are the main root causes of system issues?\")\n",
    "# Example 3: Query about specific event with chat-gpt-openai\n",
    "query_graph_llm_openai(\"What are the main root causes of system issues?\")\n",
    "# Example 3: Query about specific event with chat-llama\n",
    "query_graph_with_llm_llama(\"What are the main root causes of system issues?\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "pgVW6k59ZjAw",
    "3okTJXHOZjAw",
    "9GpDg-5FZjAw",
    "7R3KU9C5ZjAx",
    "3gTCB_QeZjAy"
   ],
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
